{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8BurEHTR_I0"
   },
   "source": [
    "# Setup Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cef0BZR1fik"
   },
   "source": [
    "Import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 163764,
     "status": "ok",
     "timestamp": 1619167146235,
     "user": {
      "displayName": "JASON RAVAGLI",
      "photoUrl": "",
      "userId": "01884994422007125599"
     },
     "user_tz": -120
    },
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup detectron2\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "import numpy as np\n",
    "import json, random, shutil\n",
    "\n",
    "import torch\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor, HookBase, launch\n",
    "from detectron2.engine import default_setup, default_argument_parser\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "import detectron2.utils.comm as comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9mU1BkRZ_f6"
   },
   "source": [
    "Global variables and settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1619167277317,
     "user": {
      "displayName": "JASON RAVAGLI",
      "photoUrl": "",
      "userId": "01884994422007125599"
     },
     "user_tz": -120
    },
    "id": "YOLsUieZaDr1"
   },
   "outputs": [],
   "source": [
    "path_train = \"/thecube/students/jravagli/datasets/train\"\n",
    "path_train_images = os.path.join(path_train, \"image\")\n",
    "path_train_json = os.path.join(path_train, \"train.json\")\n",
    "path_val_images = os.path.join(path_train, \"image\")\n",
    "path_val_json = os.path.join(path_train, \"valid.json\")\n",
    "# path_output_dir = \"/thecube/students/jravagli/outputs/detectron\"\n",
    "\n",
    "# Model settings\n",
    "lr = 0.02 # 2.5e-4 # Suggested by detectron tutorial\n",
    "batch_size = 8\n",
    "n_train_images = 163173 # Approximate number of training images\n",
    "# We make a number of iterations so as to make the model see the whole training set *epochs* times\n",
    "epochs = 12\n",
    "iterations = epochs * n_train_images // batch_size\n",
    "n_classes = 13 # Number of classes of the training set\n",
    "# LR is reduced by a gamma factor after 8 and 11 epochs\n",
    "scheduler_steps = (8*n_train_images // batch_size, 11*n_train_images // batch_size,)\n",
    "weight_decay = 1e-5\n",
    "\n",
    "resume_training = True\n",
    "num_gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not resume_training and os.path.isdir(path_output_dir):\n",
    "#     shutil.rmtree(path_output_dir)\n",
    "# os.makedirs(path_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHS1KarVWzQN"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a hook to monitor the validation loss during training ([GitHub issue](https://github.com/facebookresearch/detectron2/issues/810#issuecomment-596194293)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationLoss(HookBase):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg.clone()\n",
    "        self.cfg.DATASETS.TRAIN = cfg.DATASETS.VAL\n",
    "        self._loader = iter(build_detection_train_loader(self.cfg))\n",
    "        \n",
    "    def after_step(self):\n",
    "        data = next(self._loader)\n",
    "        with torch.no_grad():\n",
    "            loss_dict = self.trainer.model(data)\n",
    "            \n",
    "            losses = sum(loss_dict.values())\n",
    "            assert torch.isfinite(losses).all(), loss_dict\n",
    "\n",
    "            loss_dict_reduced = {\"val_\" + k: v.item() for k, v in \n",
    "                                 comm.reduce_dict(loss_dict).items()}\n",
    "            losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "            if comm.is_main_process():\n",
    "                self.trainer.storage.put_scalars(total_val_loss=losses_reduced, \n",
    "                                                 **loss_dict_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train procedure on multiple gpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(args.file_config))\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    default_setup(cfg, args)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Define model configuration\n",
    "    cfg = setup(args)\n",
    "    cfg.DATASETS.TRAIN = (\"deepfashion_train\",)\n",
    "    cfg.DATASETS.VAL = (\"deepfashion_val\",)\n",
    "    cfg.DATASETS.TEST = ()\n",
    "    cfg.DATALOADER.NUM_WORKERS = 8\n",
    "    if args.resume_training:\n",
    "        cfg.MODEL.WEIGHTS = None\n",
    "    else:\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(args.file_config)  # Let training initialize from model zoo\n",
    "\n",
    "    cfg.SOLVER.IMS_PER_BATCH = args.batch_size\n",
    "    cfg.SOLVER.MAX_ITER = args.iterations    # Number of batch updates\n",
    "    cfg.SOLVER.BASE_LR = args.lr\n",
    "    cfg.SOLVER.MOMENTUM = 0.9\n",
    "    cfg.SOLVER.GAMMA = 0.1\n",
    "    # The iteration number to decrease learning rate by GAMMA\n",
    "    cfg.SOLVER.STEPS = args.scheduler_steps\n",
    "    cfg.SOLVER.WEIGHT_DECAY = args.weight_decay\n",
    "\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # RoI batch size\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = args.n_classes\n",
    "    # DO NOT SPECIFY OUTPUT_DIR, OTHERWISE AN ERROR FOR CONFLICTUAL ACCESS TO FILES WILL BE RAISED\n",
    "#     cfg.OUTPUT_DIR = args.path_output_dir\n",
    "    \n",
    "    # Register the dataset\n",
    "    register_coco_instances(\"deepfashion_train\", {}, args.path_train_json, args.path_train_images)\n",
    "    register_coco_instances(\"deepfashion_val\", {}, args.path_val_json, args.path_val_images)\n",
    "    \n",
    "    # Train\n",
    "    trainer = DefaultTrainer(cfg)\n",
    "    \n",
    "    val_loss = ValidationLoss(cfg)  \n",
    "    trainer.register_hooks([val_loss])\n",
    "    # swap the order of PeriodicWriter and ValidationLoss\n",
    "    trainer._hooks = trainer._hooks[:-2] + trainer._hooks[-2:][::-1]\n",
    "    \n",
    "    trainer.resume_or_load(resume=args.resume_training)\n",
    "    return trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = default_argument_parser().parse_args()\n",
    "    \n",
    "    args.file_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "    args.path_train = path_train\n",
    "    args.path_train_images = path_train_images\n",
    "    args.path_train_json = path_train_json\n",
    "    args.path_val_images = path_val_images\n",
    "    args.path_val_json = path_val_json\n",
    "#     args.path_output_dir = path_output_dir\n",
    "    args.lr = lr\n",
    "    args.batch_size = batch_size\n",
    "    args.n_train_images = n_train_images\n",
    "    args.epochs = epochs\n",
    "    args.iterations = iterations\n",
    "    args.n_classes = n_classes\n",
    "    args.scheduler_steps = scheduler_steps\n",
    "    args.weight_decay = weight_decay\n",
    "    args.resume_training = resume_training\n",
    "    args.num_gpus = num_gpus\n",
    "    print(\"Command Line Args:\", args)\n",
    "    \n",
    "    launch(\n",
    "        main,\n",
    "        args.num_gpus,\n",
    "        num_machines=args.num_machines,\n",
    "        machine_rank=args.machine_rank,\n",
    "        dist_url=args.dist_url,\n",
    "        args=(args,),\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "v0dEF9HEZGmt"
   ],
   "name": "1-Mask-RCNN-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
