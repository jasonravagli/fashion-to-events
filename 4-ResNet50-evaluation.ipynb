{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, os, time, shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import sampler, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make PyTorch use the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables and settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir_project = \"/thecube/students/jravagli\"\n",
    "path_dir_output = os.path.join(path_dir_project, \"outputs\", \"resnet50-ft-final\")\n",
    "\n",
    "# Paths datasets\n",
    "path_dir_datasets = os.path.join(path_dir_project, \"datasets\", \"used-pp\")\n",
    "path_test_dataset = os.path.join(path_dir_datasets, \"test\")\n",
    "# Path annotation files\n",
    "path_test_ann = os.path.join(path_dir_datasets, \"test.txt\")\n",
    "# Path models\n",
    "path_models_folder = os.path.join(path_dir_output, \"models\")\n",
    "path_model = os.path.join(path_models_folder, \"resnet50-ft-best.pth\")\n",
    "\n",
    "model_type = \"resnet50\" # otherwise \"resnet50-embed\"\n",
    "n_classes = 14\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "\n",
    "# Resnet50 with embedding parameters\n",
    "n_embedding_feats = 10\n",
    "n_dense_units = 512\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "n_clothes_classes = 13\n",
    "orig_class_names = [\"concert\", \"graduation\", \"meeting\", \"mountain-trip\", \"picnic\",\n",
    "               \"sea-holiday\", \"ski-holiday\", \"wedding\", \"conference\", \"exhibition\", \"fashion\",\n",
    "               \"protest\", \"sport\", \"theater-dance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "size = [img_height, img_width]\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Normalize(mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom dataset that reads the data from a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsedDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.np_data = pd.read_csv(csv_file).to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.np_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path_image = self.np_data[idx, 0]\n",
    "        # Read the image in a PyTorch tensor and squeeze values in [0, 1]\n",
    "        image = read_image(path_image)/255.\n",
    "        label = self.np_data[idx, 1]\n",
    "        clothes = self.np_data[idx, 2]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\"image\": image, \"label\": label, \"clothes\": clothes, \"path\": path_image}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = UsedDataset(path_test_ann, transform=data_transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "dataset_size = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom Resnet50 model with the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility layer to delete existing layers from the pretrained network\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class TwoHeadsResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoHeadsResNet, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = Identity()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.embed = nn.Embedding(n_clothes_classes, n_embedding_feats)\n",
    "        self.fc_1 = nn.Linear(2048, n_dense_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.do = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(n_dense_units + n_embedding_feats, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, image, additional_data):\n",
    "        x_image = self.resnet(image)\n",
    "        x_image = self.flatten(x_image)\n",
    "        x_image = self.fc_1(x_image)\n",
    "        x_image = self.relu(x_image)\n",
    "        x_image = self.do(x_image)\n",
    "        x_data = self.embed(additional_data)\n",
    "        x = torch.cat((x_image, x_data), dim=1)\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build the different types of models used in the experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg16():\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    classifier = nn.Sequential(\n",
    "        nn.Linear(in_features=model.classifier[0].in_features, out_features=512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(in_features=512, out_features=512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(in_features=512, out_features=n_classes)\n",
    "    )\n",
    "    model.classifier = classifier\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_resnet50():\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    \n",
    "    # Replace the last layer with a custom classifier\n",
    "    model.fc = nn.Linear(model.fc.in_features, 512)\n",
    "    model.relu = nn.ReLU()\n",
    "    model.do = nn.Dropout(p=0.5)\n",
    "    model.fc_2 = nn.Linear(in_features=512, out_features=n_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_resnet50_embed():\n",
    "    return TwoHeadsResNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the desired model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"vgg16\":\n",
    "    model = build_vgg16()\n",
    "elif model_type == \"resnet50\":\n",
    "    model = build_resnet50()\n",
    "else:\n",
    "    model = build_resnet50_embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(path_model))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "\n",
    "all_paths = np.empty(dataset_size, dtype=object)\n",
    "all_preds = torch.zeros(dataset_size, dtype=torch.int).to(device)\n",
    "all_labels = torch.zeros(dataset_size, dtype=torch.int).to(device)\n",
    "with torch.no_grad():\n",
    "    batch_index = 0\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        images = batch_data[\"image\"].to(device)\n",
    "        clothes = batch_data[\"clothes\"].to(device)\n",
    "        labels = batch_data[\"label\"].to(device)\n",
    "        paths = batch_data[\"path\"]\n",
    "        \n",
    "        if model_type == \"resnet50-embed\":\n",
    "            outputs = model(images, clothes)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "        _, preds = torch.max(outputs,1)\n",
    "        \n",
    "        n_correct += torch.sum(preds == labels.data)\n",
    "        # Save predictions and labels for further analysis\n",
    "        all_paths[batch_size*batch_index:batch_size*(batch_index + 1)] = paths\n",
    "        all_preds[batch_size*batch_index:batch_size*(batch_index + 1)] = preds\n",
    "        all_labels[batch_size*batch_index:batch_size*(batch_index + 1)] = labels\n",
    "        \n",
    "        batch_index += 1\n",
    "        \n",
    "acc = n_correct.double() / dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels.cpu().numpy(), all_preds.cpu().numpy())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "plt.rc('axes',titlesize=18)\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=orig_class_names)\n",
    "disp.plot(xticks_rotation=\"vertical\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the normalized confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels.cpu().numpy(), all_preds.cpu().numpy(), normalize=\"true\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "plt.rc('axes',titlesize=18)\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=orig_class_names)\n",
    "disp.plot(xticks_rotation=\"vertical\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with the predictions data for a better analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"path\": all_paths, \"pred\": all_preds.cpu().numpy(), \"ground_truth\": all_labels.cpu().numpy()})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter rows with wrong predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_errors = df[~(df[\"pred\"] == df[\"ground_truth\"])]\n",
    "df_errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images where the model predicts wrong specifying the ground truth class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_class = 9\n",
    "\n",
    "df_filter = df_errors[df_errors[\"ground_truth\"] == gt_class]\n",
    "np_data = df_filter.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(len(np_data))\n",
    "data = np_data[index]\n",
    "path = data[0]\n",
    "pred = data[1]\n",
    "gt = data[2]\n",
    "\n",
    "print(f\"Ground truth: {orig_class_names[gt]} - Predicted: {orig_class_names[pred]}\")\n",
    "img = cv2.imread(path)\n",
    "plt.imshow(img[:, :, ::-1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
