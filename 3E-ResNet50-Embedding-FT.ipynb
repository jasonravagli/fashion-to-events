{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1620384407935,
     "user": {
      "displayName": "JACOPO BARTOLI",
      "photoUrl": "",
      "userId": "09877609771543953448"
     },
     "user_tz": -120
    },
    "id": "XlCuPn6kYiYb"
   },
   "outputs": [],
   "source": [
    "import copy, os, time, shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import sampler, Dataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make PyTorch use the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZkWxIzTYmpy"
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables and settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir_project = \"/thecube/students/jravagli\"\n",
    "path_dir_output = os.path.join(path_dir_project, \"outputs\", \"resnet50-embed-ft\")\n",
    "path_dir_tensorboard = os.path.join(path_dir_output, \"tensorboard\")\n",
    "\n",
    "# Paths datasets\n",
    "path_dir_datasets = os.path.join(path_dir_project, \"datasets\", \"used-pp\")\n",
    "path_train_dataset = os.path.join(path_dir_datasets, \"train\")\n",
    "path_valid_dataset = os.path.join(path_dir_datasets, \"valid\")\n",
    "path_test_dataset = os.path.join(path_dir_datasets, \"test\")\n",
    "# Path annotation files\n",
    "path_train_ann = os.path.join(path_dir_datasets, \"train.txt\")\n",
    "path_valid_ann = os.path.join(path_dir_datasets, \"valid.txt\")\n",
    "path_test_ann = os.path.join(path_dir_datasets, \"test.txt\")\n",
    "# Path models\n",
    "path_models_folder = os.path.join(path_dir_output, \"models\")\n",
    "path_model_tl = os.path.join(path_models_folder, \"resnet50-ft\")\n",
    "path_model_tl_best = os.path.join(path_models_folder, \"resnet50-ft-best.pth\")\n",
    "# Input Path\n",
    "path_input_model = os.path.join(path_dir_project, \"outputs\",\"resnet50-embed-tl-final\",\"models\",\"resnet50-tl-best.pth\")\n",
    "\n",
    "# Model parameters\n",
    "n_classes = 14\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "n_clothes_classes = 13\n",
    "n_embedding_feats = 10\n",
    "n_dense_units = 512\n",
    "\n",
    "batch_size = 64\n",
    "lr_ft = 5e-5\n",
    "ft_epochs = 32\n",
    "weight_decay = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean output dir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(path_dir_output):\n",
    "    shutil.rmtree(path_dir_output)\n",
    "\n",
    "os.mkdir(path_dir_output)\n",
    "os.mkdir(path_dir_tensorboard)\n",
    "os.mkdir(path_models_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(path_dir_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define data preprocessing operations (apply some data augmentation to the training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjPKynnG1RAy"
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset using the ImageNet mean and std\n",
    "# since we will use pretrained networks\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "size = [img_height, img_width]\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom dataset that reads the data from a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsedDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.np_data = pd.read_csv(csv_file).to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.np_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        path_image = self.np_data[idx, 0]\n",
    "        # Read the image in a PyTorch tensor and squeeze values in [0, 1]\n",
    "        image = read_image(path_image)/255.\n",
    "        label = self.np_data[idx, 1]\n",
    "        clothes = self.np_data[idx, 2]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {\"image\": image, \"label\": label, \"clothes\": clothes}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UsedDataset(path_train_ann, transform=data_transform[\"train\"])\n",
    "valid_dataset = UsedDataset(path_valid_ann, transform=data_transform[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# figure = plt.figure(figsize=(8, 8))\n",
    "# cols, rows = 3, 3\n",
    "# for i in range(1, cols * rows + 1):\n",
    "#     sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "#     data = train_dataset[sample_idx]\n",
    "#     figure.add_subplot(rows, cols, i)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(data[\"image\"].permute(1, 2, 0))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the samples weights to balance the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(data, n_classes):\n",
    "    count = [0] * n_classes\n",
    "    for item in data:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * n_classes\n",
    "    N = float(sum(count))\n",
    "    for i in range(n_classes):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(data)\n",
    "    for idx, val in enumerate(data):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data loaders. Use the WeightedRandomSampler to balance the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the unbalanced dataset we create a weighted sampler\n",
    "weights_train = make_weights_for_balanced_classes(train_dataset.np_data, n_classes)\n",
    "weights_valid = make_weights_for_balanced_classes(valid_dataset.np_data, n_classes)\n",
    "weights_train = torch.DoubleTensor(weights_train)\n",
    "weights_valid = torch.DoubleTensor(weights_valid)\n",
    "\n",
    "sampler_train = sampler.WeightedRandomSampler(weights_train, len(weights_train))\n",
    "sampler_valid = sampler.WeightedRandomSampler(weights_valid, len(weights_valid))\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                         num_workers=8, sampler=sampler_train),\n",
    "    \"valid\": torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                                         num_workers=8, sampler=sampler_valid),\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_dataset),\n",
    "    \"valid\": len(valid_dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_data in dataloaders[\"train\"]:\n",
    "#     print(f\"Batch length {len(batch_data['image'])}\")\n",
    "#     unique, counts = np.unique(batch_data[\"label\"], return_counts=True)\n",
    "#     print(f\"Classes: {unique}\")\n",
    "#     print(f\"Counts: {counts}\")\n",
    "    \n",
    "#     figure = plt.figure(figsize=(3, 3))\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.imshow(batch_data[\"image\"][0].permute(1, 2, 0))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a custom model with two inputs: an image of a clothes and its class number. The image is processed by a pretained ResNet50, while the class number is sent to an embedding layer. The resulting features are concatenated together and used by a FC classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility layer to delete existing layers from the pretrained network\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class TwoHeadsResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoHeadsResNet, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = Identity()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.embed = nn.Embedding(n_clothes_classes, n_embedding_feats)\n",
    "#         self.fc_1 = nn.Linear(2048 + n_embedding_feats, n_dense_units)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.do = nn.Dropout(p=0.5)\n",
    "#         self.fc_2 = nn.Linear(n_dense_units, n_classes)\n",
    "        self.fc_1 = nn.Linear(2048, n_dense_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.do = nn.Dropout(p=0.5)\n",
    "        self.fc_2 = nn.Linear(n_dense_units + n_embedding_feats, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, image, additional_data):\n",
    "        x_image = self.resnet(image)\n",
    "        x_image = self.flatten(x_image)\n",
    "#         x_data = self.embed(additional_data)\n",
    "#         x = torch.cat((x_image, x_data), dim=1)\n",
    "#         x = self.fc_1(x)\n",
    "        x_image = self.fc_1(x_image)\n",
    "        x_image = self.relu(x_image)\n",
    "        x_image = self.do(x_image)\n",
    "        x_data = self.embed(additional_data)\n",
    "        x = torch.cat((x_image, x_data), dim=1)\n",
    "#         x = self.do(x)\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model and load the weights from the transfer learning training. For the fine tuning process we keep the first three stages of the ResNet50 part frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_resnet_stages(model):\n",
    "    for param in model.conv1.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bn1.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.relu.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.maxpool.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer1.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer2.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer3.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model = TwoHeadsResNet()\n",
    "model.load_state_dict(torch.load(path_input_model))\n",
    "\n",
    "# Freeze the first three stages of ResNet50 before fine tuning\n",
    "print(f\"Trainable params before freeze: {count_trainable_params(model)}\")\n",
    "freeze_resnet_stages(model.resnet)\n",
    "print(f\"Trainable params after freeze: {count_trainable_params(model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, dataloaders, dataset_sizes, criterion, epoch):\n",
    "    train_loss = train_acc = val_loss = val_acc = 0\n",
    "    \n",
    "    for phase in ['train', 'valid']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        curr_batch = 0\n",
    "        for batch_data in tqdm(dataloaders[phase]):\n",
    "            images = batch_data[\"image\"].to(device)\n",
    "            clothes = batch_data[\"clothes\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(images, clothes)\n",
    "                _, preds = torch.max(outputs,1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "        \n",
    "        if phase == \"train\":\n",
    "            train_loss = epoch_loss\n",
    "            train_acc = epoch_acc\n",
    "        else:\n",
    "            val_loss = epoch_loss\n",
    "            val_acc = epoch_acc\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = np.Inf\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        loss, acc, val_loss, val_acc = do_epoch(model, dataloaders, dataset_sizes, criterion, epoch)\n",
    "        \n",
    "        print(f\"Train Loss: {loss:.4f} - Train Acc: {acc:.4f}\")\n",
    "        print(f\"Valid Loss: {val_loss:.4f} - Valid Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        writer.add_scalar('Training loss', loss, epoch)\n",
    "        writer.add_scalar('Training accuracy', acc, epoch)\n",
    "        writer.add_scalar('Valid loss', val_loss, epoch)\n",
    "        writer.add_scalar('Valid accuracy', val_acc, epoch)\n",
    "        \n",
    "        # Save best model monitoring validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), path_model_tl_best)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    h = time_elapsed//3600\n",
    "    m = time_elapsed//60 - h*60\n",
    "    s = time_elapsed%60\n",
    "    print(f'Training complete in {h}h {m}m {s}s')\n",
    "    print(f'Best Valid Acc: {best_val_acc:4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQYEvIC17u4-"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr_ft, weight_decay=weight_decay)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, dataloaders,\n",
    "                    dataset_sizes, num_epochs=ft_epochs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFE7pjyZrUYy2q+UQnH8M8",
   "collapsed_sections": [],
   "name": "pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
